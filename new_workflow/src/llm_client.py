# llm_client.py
import asyncio
import base64
import os
from google import genai
from google.genai import types
import time
from pathlib import Path
from typing import Optional, Union, List
from .config_loader import get_config

# ç¦ç”¨ gemini_webapi è¯¦ç»†æ—¥å¿—
try:
    from gemini_webapi import set_log_level
    set_log_level("ERROR")
except ImportError:
    pass

from dataclasses import dataclass

PROXY_URL = get_config("proxy.url", "")
if PROXY_URL:
    os.environ["HTTP_PROXY"] = PROXY_URL
    os.environ["HTTPS_PROXY"] = PROXY_URL

from dataclasses import dataclass
@dataclass
class ChatResponse:
    """ç»Ÿä¸€çš„è¿”å›ç»“æœå¯¹è±¡"""
    text: str
    saved_images: List[str]
    raw_response: object


class LLMClient:
    """
    ç»Ÿä¸€çš„å¤§æ¨¡å‹äº¤äº’å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Google GenAIã€OpenAIã€æ™ºè°±AI å’Œ Gemini Web APIã€‚
    è®¾è®¡ç”¨äºæ–¹ä¾¿æ‰©å±•å…¶ä»–æ¨¡å‹æ¥å£ã€‚
    
    æ”¯æŒçš„æä¾›å•†:
        - gemini: Google GenAI å®˜æ–¹ API
        - gemini_web: Gemini Web API (åŸºäºæµè§ˆå™¨ Cookieï¼Œæ”¯æŒå›¾ç‰‡ç”Ÿæˆ)
        - openai: OpenAI å…¼å®¹æ¥å£ (æ”¯æŒ OpenRouter)
        - zhipu: æ™ºè°±AI
    """
    
    # æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
    IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.webp', '.gif', '.heic', '.heif'}
    
    def __init__(self, provider=None, api_key=None, model=None, temperature=None):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            provider (str): æä¾›å•†ç±»å‹ï¼Œæ”¯æŒ "gemini"ã€"gemini_web"ã€"openai" æˆ– "zhipu"
            api_key (str, optional): API å¯†é’¥ (gemini_web ä¸éœ€è¦)
            model (str, optional): æ¨¡å‹åç§°
            temperature (float, optional): æ¸©åº¦å‚æ•°
        """
        if provider is None:
            provider = "zhipu"
        self.provider = provider.lower()
        
        self.temperature = temperature if temperature is not None else get_config("api.default_temperature", 0.2)
        self.max_retries = get_config("api.max_retries", 3)
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ–
        self._init_provider(api_key, model)

    def _init_provider(self, api_key: str, model: str):
        """æ ¹æ®æä¾›å•†ç±»å‹åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        
        if self.provider == "gemini":
            self._init_gemini(api_key, model)
            
        elif self.provider == "gemini_web":
            self._init_gemini_web(model)
            
        elif self.provider == "openai":
            self._init_openai(api_key, model)
            
        elif self.provider == "zhipu":
            self._init_zhipu(api_key, model)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {self.provider}")

    def _init_gemini(self, api_key: str, model: str):
        """åˆå§‹åŒ– Gemini å®˜æ–¹ API"""
        self.api_key = api_key or get_config("api.genai_key")
        self.base_url = get_config("api.genai_base_url", None)
        self.model = model or get_config("api.genai_model", "gemini-flash-lite-latest")
        
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° Gemini API å¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        if self.base_url:
            self.client = genai.Client(api_key=self.api_key, http_options={"base_url": self.base_url})
        else:
            self.client = genai.Client(api_key=self.api_key)

    def _init_gemini_web(self, model: str):
        """åˆå§‹åŒ– Gemini Web API (å»¶è¿Ÿåˆå§‹åŒ–ï¼Œå¼‚æ­¥ä½¿ç”¨æ—¶æ‰åˆ›å»º)"""
        from gemini_webapi.constants import Model
        self.model = model or Model.UNSPECIFIED
        self.proxy = get_config("proxy.url", None)
        # Web API å®¢æˆ·ç«¯éœ€è¦å¼‚æ­¥åˆå§‹åŒ–ï¼Œè¿™é‡Œåªåšæ ‡è®°
        self.client = None
        self._gemini_web_initialized = False

    def _init_openai(self, api_key: str, model: str):
        """åˆå§‹åŒ– OpenAI å…¼å®¹æ¥å£"""
        self.api_key = api_key or get_config("api.openai_key")
        self.base_url = get_config("api.openai_base_url", "https://openrouter.ai/api/v1/")
        self.model = model or get_config("api.openai_model", "gemini-flash-lite-latest")
        
        if not self.api_key or self.api_key == "your-openai-key":
            raise ValueError("æœªæ‰¾åˆ° OpenAI API å¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        try:
            from openai import OpenAI
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai åº“: pip install openai")

    def _init_zhipu(self, api_key: str, model: str):
        """åˆå§‹åŒ–æ™ºè°±AI"""
        self.api_key = api_key or get_config("api.zhipu_key")
        self.model = model or get_config("api.zhipu_model", "glm-4.5-flash")
        self.enable_thinking = get_config("api.zhipu_enable_thinking", True)
        
        if not self.api_key or self.api_key == "your-zhipu-key":
            raise ValueError("æœªæ‰¾åˆ°æ™ºè°±AI API å¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        try:
            from zai import ZhipuAiClient
            self.client = ZhipuAiClient(api_key=self.api_key)
        except ImportError:
            raise ImportError("è¯·å®‰è£… zai åº“: pip install zai")

    def _normalize_file_paths(self, file_path: Union[str, List[str], None]) -> List[str]:
        """ç»Ÿä¸€å¤„ç†æ–‡ä»¶è·¯å¾„å‚æ•°ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼"""
        if file_path is None:
            return []
        if isinstance(file_path, str):
            return [file_path]
        return file_path

    # ==================== ç»Ÿä¸€æ¥å£ ====================
    
    def generate(self, prompt: str, file_path: Union[str, List[str], None] = None) -> str:
        """
        ç»Ÿä¸€ç”Ÿæˆæ¥å£ï¼ˆåŒæ­¥ï¼‰ï¼Œè‡ªåŠ¨æ ¹æ® provider è°ƒç”¨å¯¹åº”æ–¹æ³•
        
        æ³¨æ„: gemini_web æä¾›å•†éœ€è¦ä½¿ç”¨ generate_async() æ–¹æ³•
        """
        if self.provider == "gemini":
            return self.generate_with_gemini(prompt, file_path)
        elif self.provider == "gemini_web":
            # å¯¹äº gemini_webï¼Œåœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
            return asyncio.run(self.generate_async(prompt, file_path))
        elif self.provider == "openai":
            return self.generate_with_openai(prompt, file_path)
        elif self.provider == "zhipu":
            return self.generate_with_zhipu(prompt, file_path)

    async def generate_async(
        self, 
        prompt: str, 
        file_path: Union[str, List[str], None] = None,
        save_img_path: str = None,
        new_chat: bool = False
    ) -> Union[str, "ChatResponse"]:
        """
        ç»Ÿä¸€ç”Ÿæˆæ¥å£ï¼ˆå¼‚æ­¥ï¼‰ï¼Œæ”¯æŒæ‰€æœ‰æä¾›å•†
        
        Args:
            prompt: æç¤ºè¯
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªæˆ–åˆ—è¡¨ï¼‰
            save_img_path: å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼ˆä»… gemini_web æ”¯æŒï¼‰
            new_chat: æ˜¯å¦å¼€å¯æ–°ä¼šè¯ï¼ˆä»… gemini_web æ”¯æŒï¼‰
            
        Returns:
            str: å½“ save_img_path ä¸º None æ—¶è¿”å›çº¯æ–‡æœ¬
            ChatResponse: å½“ save_img_path ä¸ä¸º None æ—¶è¿”å›å®Œæ•´å“åº”å¯¹è±¡ (gemini_web)
        """
        if self.provider == "gemini_web":
            response = await self.generate_with_gemini_web(prompt, file_path, save_img_path, new_chat)
            # å¦‚æœæœªæŒ‡å®šå›¾ç‰‡ä¿å­˜è·¯å¾„ï¼Œåªè¿”å›æ–‡æœ¬å†…å®¹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            if save_img_path is None:
                return response.text
            return response
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨åŒæ­¥æ–¹æ³•åŒ…è£…
            return self.generate(prompt, file_path)

    # ==================== Gemini Web API ====================
    
    async def generate_with_gemini_web(
        self, 
        prompt: str, 
        file_path: Union[str, List[str], None] = None,
        save_img_path: str = None,
        new_chat: bool = False
    ) -> "ChatResponse":
        """
        ä½¿ç”¨ Gemini Web API ç”Ÿæˆå†…å®¹ï¼ˆåŸºäºæµè§ˆå™¨ Cookieï¼‰
        
        ç‰¹ç‚¹:
            - æ”¯æŒå›¾ç‰‡ç”Ÿæˆå¹¶è‡ªåŠ¨ä¸‹è½½
            - æ”¯æŒå¤šè½®å¯¹è¯
            - æ”¯æŒæ–‡ä»¶ä¸Šä¼ åˆ†æ
        
        Args:
            prompt: æç¤ºè¯
            file_path: æ–‡ä»¶è·¯å¾„
            save_img_path: å›¾ç‰‡ä¿å­˜ç›®å½•ï¼ˆå¦‚éœ€ç”Ÿæˆå›¾ç‰‡ï¼‰
            new_chat: æ˜¯å¦å¼€å¯æ–°ä¼šè¯
            
        Returns:
            ChatResponse: åŒ…å« textã€saved_imagesã€raw_response
        """
        from gemini_webapi import GeminiClient
        from gemini_web.get_cookie import get_gemini_tokens
        from .config_loader import get_config
        from .logger import logger
        
        file_paths = self._normalize_file_paths(file_path)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        for fp in file_paths:
            if not os.path.exists(fp):
                return ChatResponse(text=f"Error: File not found: {fp}", saved_images=[], raw_response=None)

        # å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯
        if not self._gemini_web_initialized:
            cookie_file = get_config("api.gemini_cookie_file", None)
            
            # é‡è¯•é€»è¾‘
            for attempt in range(self.max_retries):
                try:
                    token_id, token_ts = get_gemini_tokens(cookie_file)
                    self.client = GeminiClient(token_id, token_ts, proxy=self.proxy)
                    
                    # å¢åŠ è¶…æ—¶æ—¶é—´å¹¶æ·»åŠ é‡è¯•
                    await self.client.init(timeout=60, auto_refresh=True)
                    self.chat_session = self.client.start_chat(model=self.model)
                    self._gemini_web_initialized = True
                    logger.info("âœ… Gemini Web API å·²å°±ç»ª")
                    break
                except Exception as e:
                    logger.warning(f"Gemini Web åˆå§‹åŒ–å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(2 * (attempt + 1))
                    else:
                        raise RuntimeError(f"Gemini Web åˆå§‹åŒ–å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")

        # å¼€å¯æ–°ä¼šè¯
        if new_chat:
            self.chat_session = self.client.start_chat(model=self.model)
            logger.info("ğŸ”„ --- å¼€å¯æ–°ä¼šè¯ ---")

        # å‘é€æ¶ˆæ¯
        response = await self.chat_session.send_message(prompt, files=file_paths or [])
        
        saved_paths = []
        if save_img_path:
            saved_paths = await self._process_and_save_images_web(response, save_img_path, logger)

        return ChatResponse(
            text=response.text,
            saved_images=saved_paths,
            raw_response=response
        )

    async def _process_and_save_images_web(self, response, save_dir: str, logger) -> List[str]:
        """å¤„ç† Gemini Web API è¿”å›çš„å›¾ç‰‡å¹¶ä¿å­˜"""
        # æœé›†æ‰€æœ‰å›¾ç‰‡å¯¹è±¡
        images = list(response.images)
        if not images and response.candidates:
            for candidate in response.candidates:
                if candidate.images:
                    for img in candidate.images:
                        if img.url not in [i.url for i in images]:
                            images.append(img)
        
        if not images and "image_generation_content" in response.text:
            logger.warning("âš ï¸ è­¦å‘Š: æ£€æµ‹åˆ°å›¾ç‰‡å ä½ç¬¦ä½†æœªè§£æåˆ°å¯¹è±¡ (Google API æ³¢åŠ¨)")

        if not images:
            return []

        Path(save_dir).mkdir(parents=True, exist_ok=True)
        
        logger.info(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ {len(images)} å¼ å›¾ç‰‡...")
        tasks = []
        paths = []
        
        for i, img in enumerate(images):
            clean_title = "".join(c for c in (img.title or "gen") if c.isalnum())[:15]
            filename = f"{clean_title}_{i}_{int(time.time())}.png"
            full_path = os.path.join(save_dir, filename)
            paths.append(full_path)
            tasks.append(img.save(path=save_dir, filename=filename))

        await asyncio.gather(*tasks, return_exceptions=True)
        return paths

    async def close_gemini_web(self):
        """å…³é—­ Gemini Web API è¿æ¥"""
        if self.provider == "gemini_web" and self._gemini_web_initialized:
            await self.client.close()
            self._gemini_web_initialized = False
            from .logger import logger
            logger.info("ğŸ’¤ Gemini Web API å·²å…³é—­")

    # ==================== Context Manager æ”¯æŒ ====================
    
    async def __aenter__(self):
        """æ”¯æŒ async with è¯­æ³•"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """è‡ªåŠ¨å…³é—­è¿æ¥"""
        await self.close_gemini_web()

    # ==================== å…¶ä»–æä¾›å•†æ–¹æ³• (ä¿æŒä¸å˜) ====================

    def generate_with_gemini(self, prompt: str, file_path: Union[str, List[str], None] = None) -> str:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆå†…å®¹ã€‚"""
        file_paths = self._normalize_file_paths(file_path)
        
        for attempt in range(self.max_retries):
            try:
                parts = []
                
                for fp in file_paths:
                    if not os.path.exists(fp):
                        return f"Error: File not found: {fp}"
                    
                    ext_to_mime = {
                        '.pdf': 'application/pdf',
                        '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',
                        '.png': 'image/png', '.webp': 'image/webp',
                        '.heic': 'image/heic', '.heif': 'image/heif',
                        '.mp4': 'video/mp4', '.mpeg': 'video/mpeg',
                        '.mov': 'video/mov', '.avi': 'video/avi',
                        '.flv': 'video/x-flv', '.mpg': 'video/mpg',
                        '.webm': 'video/webm', '.wmv': 'video/wmv',
                        '.3gpp': 'video/3gpp',
                    }
                    
                    file_ext = os.path.splitext(fp)[1].lower()
                    mime_type = ext_to_mime.get(file_ext)
                    
                    if not mime_type:
                        return f"Error: Unsupported file type: {file_ext}"
                    
                    try:
                        with open(fp, "rb") as f:
                            file_data = f.read()
                        parts.append(types.Part.from_bytes(
                            mime_type=mime_type,
                            data=base64.b64decode(base64.b64encode(file_data).decode())
                        ))
                    except PermissionError:
                        return f"Error: Permission denied: {fp}"
                    except Exception as e:
                        return f"Error reading file {fp}: {e}"

                parts.append(types.Part.from_text(text=prompt))
                contents = [types.Content(role="user", parts=parts)]
                
                config = types.GenerateContentConfig(
                    temperature=self.temperature,
                    thinking_config=types.ThinkingConfig(thinking_budget=-1),
                )

                try:
                    response_text = ""
                    for chunk in self.client.models.generate_content_stream(
                        model=self.model,
                        contents=contents,
                        config=config,
                    ):
                        response_text += chunk.text
                    return response_text
                except Exception as e:
                    return f"LLM Generation Error: {e}"
            except Exception as e:
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return f"LLM Generation Error after {self.max_retries} attempts: {e}"

    def generate_with_openai(self, prompt: str, file_path: Union[str, List[str], None] = None) -> str:
        """ä½¿ç”¨æ ‡å‡† OpenAI èŠå¤©æ¥å£ç”Ÿæˆå†…å®¹ï¼Œæ”¯æŒ OpenRouterã€‚"""
        from .pdf_to_markdown import convert_pdf_to_markdown

        file_paths = self._normalize_file_paths(file_path)
        content = [{"type": "text", "text": prompt}]
        
        for fp in file_paths:
            if not os.path.exists(fp):
                return f"Error: File not found: {fp}"
            
            file_ext = os.path.splitext(fp)[1].lower()
            
            if file_ext in self.IMAGE_EXTENSIONS:
                try:
                    with open(fp, "rb") as image_file:
                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    
                    mime_map = {'.jpg': 'jpeg', '.jpeg': 'jpeg', '.png': 'png', '.webp': 'webp', '.gif': 'gif'}
                    mime_type = f"image/{mime_map.get(file_ext, 'jpeg')}"
                    
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}
                    })
                except Exception as e:
                    return f"Error encoding image {fp}: {e}"
            
            elif file_ext == '.pdf':
                try:
                    markdown_text = convert_pdf_to_markdown(fp)
                    content[0]["text"] += f"\n\n--- [File: {os.path.basename(fp)} å†…å®¹å¼€å§‹] ---\n{markdown_text}\n--- [å†…å®¹ç»“æŸ] ---"
                except Exception as e:
                    return f"Error parsing PDF {fp}: {e}"
            else:
                return f"Error: OpenAI æ¥å£æš‚ä¸æ”¯æŒç›´æ¥ä¸Šä¼  {file_ext} ç±»å‹æ–‡ä»¶ã€‚"

        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": content}],
                    temperature=self.temperature,
                )
                return response.choices[0].message.content
            
            except Exception as e:
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return f"OpenAI Generation Error after {self.max_retries} attempts: {e}"
                
    def generate_with_zhipu(self, prompt: str, file_path: Union[str, List[str], None] = None) -> str:
        """ä½¿ç”¨æ™ºè°±AI API ç”Ÿæˆå†…å®¹ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€‚"""
        from .pdf_to_markdown import convert_pdf_to_markdown

        file_paths = self._normalize_file_paths(file_path)
        
        for attempt in range(self.max_retries):
            try:
                content_parts = []
                
                for fp in file_paths:
                    if not os.path.exists(fp):
                        return f"Error: File not found: {fp}"
                    
                    file_ext = os.path.splitext(fp)[1].lower()
                    
                    if file_ext in self.IMAGE_EXTENSIONS:
                        with open(fp, "rb") as f:
                            file_data = base64.b64encode(f.read()).decode("utf-8")
                        content_parts.append({
                            "type": "image_url",
                            "image_url": {"url": file_data}
                        })
                    elif file_ext == '.pdf':
                        try:
                            markdown_text = convert_pdf_to_markdown(fp)
                            prompt += f"\n\n--- [File: {os.path.basename(fp)} å†…å®¹å¼€å§‹] ---\n{markdown_text}\n--- [å†…å®¹ç»“æŸ] ---"
                        except Exception as e:
                            return f"Error parsing PDF {fp}: {e}"
                                   
                content_parts.append({"type": "text", "text": prompt})
                
                messages = [{"role": "user", "content": content_parts}]
                request_params = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature,
                    "stream": True,
                }
                
                if self.enable_thinking:
                    request_params["thinking"] = {"type": "enabled"}
                
                response = self.client.chat.completions.create(**request_params)
                
                response_text = ""
                for chunk in response:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content:
                            response_text += delta.content
                
                if not response_text:
                    return "Warning: APIè¿”å›ç©ºå“åº”ã€‚è¯·æ£€æŸ¥æ–‡ä»¶URLæ˜¯å¦ä¸ºå®˜æ–¹åŸŸå(å¦‚cdn.bigmodel.cn)"
                
                return response_text
                    
            except Exception as e:
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return f"LLM Generation Error: {e}"


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    import asyncio
    
    async def test_gemini_web():
        """æµ‹è¯• Gemini Web API"""
        print("\n--- æµ‹è¯• Gemini Web API ---")
        
        async with LLMClient(provider="gemini_web") as llm:
            # æµ‹è¯•1: çº¯æ–‡æœ¬å¯¹è¯
            response = await llm.generate_async("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±")
            print(f"å›å¤: {response.text[:100]}...")
            
            # æµ‹è¯•2: å›¾ç‰‡ç”Ÿæˆ
            response = await llm.generate_async(
                "ç”Ÿæˆä¸€åªå¯çˆ±çš„çŒ«å’ªå›¾ç‰‡",
                save_img_path="downloads/test_cat"
            )
            if response.saved_images:
                print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {response.saved_images}")
            
            # æµ‹è¯•3: æ–‡ä»¶åˆ†æ
            pdf_path = "path/to/your/file.pdf"
            if os.path.exists(pdf_path):
                response = await llm.generate_async(
                    "æ€»ç»“è¿™ä¸ªæ–‡ä»¶çš„ä¸»è¦å†…å®¹",
                    file_path=pdf_path,
                    new_chat=True
                )
                print(f"æ€»ç»“: {response.text[:200]}...")

    # è¿è¡Œæµ‹è¯•
    # asyncio.run(test_gemini_web())
    
    # åŒæ­¥æ–¹å¼ä½¿ç”¨ (ä¼šè‡ªåŠ¨è¿è¡Œ asyncio.run)
    # llm = LLMClient(provider="gemini_web")
    # result = llm.generate("ä½ å¥½")
    # print(result)